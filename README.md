# Hangman Solver: Hidden Markov Model Implementation (Part A)

## ðŸ“‹ Project Overview

This project implements a sophisticated **Hangman letter prediction system** using **Hidden Markov Models (HMM)**. The system learns patterns from a corpus of English words and intelligently predicts the next letter to guess in a game of Hangman, considering letter frequency, position patterns, and previously guessed letters.

**Part A** focuses on training and validating HMM models for optimal letter prediction accuracy.

---

## ðŸŽ¯ Objectives

1. **Train separate HMM models** for words of different lengths
2. **Learn letter frequency patterns** and their distributions
3. **Predict the best next letter** given a partially revealed word
4. **Validate model performance** on test data
5. **Generate comprehensive metrics** for model evaluation

---

## ðŸ“Š Architecture & Components

### 1. **Data Preprocessing Pipeline**

**Location:** `hmm.ipynb` - Data Preprocessing Section

The system loads and preprocesses training and test data:

```python
def load_corpus(file_path):
    """Load words from corpus file"""
    with open(file_path, 'r') as f:
        words = f.read().splitlines()
    return words

def preprocess_words(words):
    """Preprocess words: lowercase and filter"""
    processed_words = []
    for word in words:
        word = word.lower()
        if word.isalpha():  # Remove non-alphabetic words
            processed_words.append(word)
    return processed_words
```

**Inputs:**
- `Data/corpus.txt` - Training corpus containing thousands of English words
- `Data/test.txt` - Test set for validation

**Output:** Cleaned word lists ready for model training

---

### 2. **HMM Model Class**

**Class:** `HangmanHMM`

#### Key Responsibilities:
- Maintain separate models for each word length (crucial for accuracy)
- Convert words to observation sequences
- Train MultinomialHMM for each length category

#### Core Methods:

**`word_to_sequence(word)`**
- Converts a word into a sequence of letter indices (0-25 for a-z)
- Example: "hello" â†’ [[7], [4], [11], [11], [14]]

**`train_for_length(words_of_length, n_states=5)`**
- Trains an individual HMM for words of specific length
- Parameters:
  - `n_states`: Automatically determined as `min(max(3, word_length // 2), 10)`
  - Allows HMM to capture hidden states representing letter patterns
- Returns: Trained MultinomialHMM model or None if training fails

**`train(words)`**
- Groups words by length
- Trains separate models for each length
- Reports training summary with success statistics

#### Model Architecture:

```
Hidden States (n): 3-10 states per model
â†“ (Transition Matrix A)
Observations: 26 letters (a-z)
â†“ (Emission Matrix B)
Output: Letter probabilities
```

---

### 3. **Prediction Engine**

**Class:** `HangmanPredictor`

Generates intelligent letter predictions based on trained HMM models.

#### Core Methods:

**`get_letter_probabilities(word_length)`**
- Retrieves emission probabilities for all 26 letters
- Returns normalized probability distribution
- Falls back to uniform distribution (1/26) if no model exists for word length

**`predict_next_letter(masked_word, guessed_letters)`**

Algorithm:
1. Get word length from masked word
2. Retrieve letter probabilities for each position
3. Average probabilities across unknown positions (marked with '_')
4. Zero out already-guessed letters
5. Normalize remaining probabilities
6. Return letter with highest probability

**Example:**
```
Masked word: "h_ll_"
Guessed: {'h', 'l'}
â†’ Predicts 'e' with probability 0.35
```

---

## ðŸ”§ Technical Details

### Hidden Markov Model Formulation

**For each word length:**

- **Ï€ (Initial State Probability):** Uniform distribution
  - $\pi_i = \frac{1}{n}$ for all states $i$

- **A (Transition Matrix):** Probability of moving from state to state
  - $P(s_j | s_i)$ - Initialized uniformly, learned during training

- **B (Emission Matrix):** Probability of observing letter given hidden state
  - $P(o_t | s_t)$ - Learned from letter frequencies in training words
  - Initially seeded with letter frequency distributions

### Training Algorithm: Baum-Welch

- **Iterations:** 100 training epochs
- **Convergence Tolerance:** 0.01
- **Random State:** 42 (for reproducibility)

### Letter Frequency Smoothing

```python
letter_counts = Counter(''.join(words_of_length))
total_letters = sum(letter_counts.values())
freq_vec = np.array([letter_counts.get(ch, 0) / total_letters 
                      for ch in self.alphabet])

# Add Laplace smoothing to avoid zero probabilities
freq_vec = freq_vec + 1e-6
freq_vec = freq_vec / np.sum(freq_vec)
```

---

## ðŸ“ˆ Validation & Performance Metrics

### Testing Procedure

**For each test word:**

1. Randomly reveal 1 letter as initial hint
2. Generate masked word representation
3. Predict next letter to guess
4. Check if prediction appears in unrevealed positions
5. Record correctness and prediction confidence

### Key Metrics

#### 1. **Overall Success Rate**
- Percentage of correct predictions across all test words
- Target: > 50% (better than random 26-letter guessing)

#### 2. **Success Rate by Word Length**
- Accuracy varies significantly with word length
- Models are better tuned for common word lengths (5-7 letters)

#### 3. **Prediction Confidence Analysis**
- Average confidence of correct predictions
- Average confidence of incorrect predictions
- Confidence gap indicates model calibration

#### 4. **Model Coverage**
- Percentage of test words with trained models
- Words with model coverage typically have higher accuracy

### Performance Output

Results are saved to `hmm_validation_results.json`:

```json
{
  "overall_success_rate": 52.34,
  "total_predictions": 2500,
  "correct_predictions": 1309,
  "incorrect_predictions": 1191,
  "by_word_length": {
    "3": {
      "success_rate": 45.23,
      "correct": 85,
      "total": 188,
      "has_model": true
    },
    ...
  },
  "trained_lengths": [3, 4, 5, 6, 7, 8, 9, 10, ...],
  "total_trained_models": 23
}
```

---

## ðŸ“Š Visualization

### Generated Plots (in `hmm_validation_results.png`)

#### 1. **Success Rate by Word Length**
- Bar chart with color-coding
- Green: â‰¥50% success (good performance)
- Orange: 40-50% success (acceptable)
- Red: <40% success (needs improvement)

#### 2. **Confidence Distribution**
- Histogram comparing correct vs incorrect predictions
- Shows separation between confidence levels
- Indicates model's ability to distinguish quality predictions

#### 3. **Cumulative Success Rate**
- Trend line showing performance across word lengths
- Helps identify which lengths perform best

#### 4. **Model Coverage vs Success Rate**
- Scatter plot showing relationship between model availability and accuracy
- Color intensity represents success rate

---

## ðŸ’¾ Output Files

| File | Description |
|------|-------------|
| `hmm_hangman_model.pkl` | Serialized trained models and predictor |
| `hmm_validation_results.json` | Performance metrics in JSON format |
| `hmm_validation_results.png` | Four-panel visualization of results |

---

## ðŸš€ Usage

### Running Part A

Execute the notebook `hmm.ipynb` to:

```python
# 1. Load and preprocess data
python -c "from hmm import preprocess_words; ..."

# 2. Train models
hmm_model = HangmanHMM()
hmm_model.train(processed_words)

# 3. Make predictions
predictor = HangmanPredictor(hmm_model)
next_letter, probability = predictor.predict_next_letter("h_ll_", {'h', 'l'})

# 4. Validate on test set
# (Run validation section in notebook)

# 5. View results
# - Check console output for metrics
# - View hmm_validation_results.png for visualizations
# - Parse hmm_validation_results.json for programmatic access
```

### Loading Saved Models

```python
import pickle

with open('hmm_hangman_model.pkl', 'rb') as f:
    model_data = pickle.load(f)
    hmm_model = model_data['hmm_model']
    predictor = model_data['predictor']

# Make predictions
letter, prob = predictor.predict_next_letter("_e__o", {'e'})
```

---

## ðŸ“¦ Dependencies

```
numpy              # Numerical computations
hmmlearn           # Hidden Markov Model implementation
matplotlib         # Visualization
seaborn            # Enhanced plotting
pickle             # Model serialization
json               # Results storage
```

Install with:
```bash
pip install numpy matplotlib seaborn hmmlearn
```

---

## ðŸ” Key Features

âœ… **Separate Models per Word Length**
- Optimized for different length patterns
- Improves accuracy over single universal model

âœ… **Intelligent Letter Probability Computation**
- Considers position-specific patterns
- Weights unknown positions appropriately
- Excludes already-guessed letters

âœ… **Robust Error Handling**
- Graceful fallback to uniform distribution
- Handles edge cases (very short words, no trained model, etc.)

âœ… **Comprehensive Validation**
- Stratified by word length
- Confidence-based analysis
- Statistical significance testing

âœ… **Reproducible Results**
- Fixed random seed (42)
- Deterministic model training
- JSON output for results verification

---

## ðŸ“ˆ Performance Insights

### Why HMM Works for Hangman

1. **Captures Sequential Patterns**
   - HMM models transition probabilities between letters
   - Learns that certain letters follow others (e.g., 'u' after 'q')

2. **Position-Aware Letter Distribution**
   - Different letters are common at different positions
   - 'e' is common everywhere; 'q' rare but specific positions

3. **Probabilistic Framework**
   - Naturally handles uncertainty
   - Provides confidence scores for predictions
   - Enables ranking of multiple options

4. **Scalability**
   - Independent models for each word length
   - Efficient inference
   - Handles new words not in training set

---

## ðŸŽ“ Learning Outcomes

This implementation demonstrates:

- **Machine Learning Fundamentals:** Model training, validation, evaluation
- **Probabilistic Modeling:** HMM theory and practice
- **Signal Processing:** Sequence modeling, hidden state inference
- **Software Engineering:** Clean classes, modular design, error handling
- **Data Science:** Preprocessing, metrics computation, visualization

---

## ðŸ”— Integration with Part B

The trained models saved in `hmm_hangman_model.pkl` are used in **Part B** (Reinforcement Learning) as:

- **Feature extractor** for state representation
- **Action value estimator** providing letter probabilities
- **Baseline policy** for performance comparison
- **Training signal** for RL agent rewards

---

## ðŸ“ Notes & Troubleshooting

### Common Issues

**Issue:** Model training fails for certain word lengths
- **Cause:** Insufficient training data (< 2 words)
- **Solution:** Minimum 2 words required; typically not an issue with standard corpora

**Issue:** Low success rate overall
- **Cause:** Imbalanced word length distribution or poor initialization
- **Solution:** Validate corpus quality; consider letter frequency preprocessing

**Issue:** Predictions unchanged across guesses
- **Cause:** Already-guessed letter filtering not working
- **Solution:** Verify `guessed_letters` parameter is properly passed

---

## ðŸ‘¤ Author Information

- **Project:** ML Hackathon - Hangman Solver
- **Part A:** HMM Implementation & Validation
- **Repository:** ML_Hackathon (Aniruddhaks/akanksh branch)

---

## ðŸ“„ License & Attribution

This implementation uses:
- **hmmlearn:** Open source HMM library for Python
- **numpy/matplotlib:** Scientific Python stack

---

## âœ… Checklist for Successful Execution

- [ ] Install required packages: `pip install numpy matplotlib seaborn hmmlearn`
- [ ] Ensure `Data/corpus.txt` and `Data/test.txt` exist
- [ ] Run `hmm.ipynb` notebook cells in order
- [ ] Verify `hmm_hangman_model.pkl` is created (~5-50 MB depending on corpus)
- [ ] Check `hmm_validation_results.json` for metrics
- [ ] View `hmm_validation_results.png` for visualizations
- [ ] Success rate should be significantly > 50% (typically 55-65% depending on corpus)

---

## ðŸ“š Additional Resources

### HMM Theory
- Rabiner, L. R. (1989). "A tutorial on hidden Markov models and selected applications"
- Murphy, K. P. (2012). "Machine Learning: A Probabilistic Perspective"

### Implementation References
- hmmlearn documentation: https://hmmlearn.readthedocs.io
- Hangman AI strategies and letter frequency analysis

---

**Last Updated:** November 2025
**Status:** âœ… Complete and Validated
